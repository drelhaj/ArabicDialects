\documentclass[11pt]{article}
\usepackage{subcaption}
\usepackage{times}
\usepackage{arabtex}
\usepackage{latexsym}
\usepackage{url}
\usepackage{utf8}
\usepackage{coling2016}
\usepackage{xfrac}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Arabic Dialect Identification using Machine Learning and Corpus Linguistics}

\author{Mahmoud El-Haj and Paul Rayson \\
  School of Computing and Communications \\
  InfoLab21, Lancaster University\\
  {\tt \{m.el-haj, prayson\}@lancaster.ac.uk}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
In this paper we experiment with Dialect Identification DID using Machine Learning and Code Switching
\end{abstract}

\section{Introduction}
\label{intro}
% Paul to add importance of corpus linguistics
In natural language processing the problem of detecting the language of a given textual content is called language identification or language guessing. Most of computer application refer to this as a classification problem where machine learning can be used to distinguish between languages \cite{Gupta(2014)}. In previous years approaches using character n-grams and basic corpus-linguistics were more common than machine learning especially that character n-gram has been proven to work well in the language identification process \cite{Cavnar(1994),Dunning(1994),Souter(1994)}.
In this paper we work on another level of language identification by focusing on dialects of one language taking Arabic as our case study. The work present our participation at the DSL shared task\footnote{VarDial 2016 - Workshop on NLP for Similar Languages, Varieties and Dialects \\ \url{http://ttg.uni-saarland.de/vardial2016/dsl2016.html}} on language identification and this year they introduce and for the first time a task on Arabic dialects identification.

\section{Related Work}
% Paul to do
% Language ID in general
% Dialect ID specifically

\section{Dataset}
The dataset used in our work was generated automatically using an Arabic Automatic Speech Recognition (ASR) system \cite{Ali(2014),Ali(2015)}. The data was provided by the DSL shared task organiser. The training training corpus was collected from the Broadcast News domain in four Arabic dialects Egyptian (EGY) Valentine (LAV), Gulf (GLF), and North (NOR) with the latter referring to North Africa Arabic speaking countries apart from Egypt. The process included Modern Standard Arabic (MSA) as well \cite{Ali(2014),Ali(2015)}.
We noticed a high frequency of MSA vocabularies and phrases within the dialects dataset. The dataset appears to be a combination of dialect speakers and those who try to speak MSA (speakers on news channels for example), which resulted in phrases such as ``\<.htY h_dh Al-l.h.zT>'' [Until this moment] and ``\<lm ysbq An>'' [never before]. We refer to this as dialectal code switching between MSA and the remaining Arabic dialects and we will later show how this played a vital role in the dialect identification process.
The dataset was labeled with one of the following classes (EGY, GLF, LAV, NOR and MSA). Table \ref{tab:trainingLabels} shows the count of instances (samples) for each class.

\begin{table}[ht]
  \centering
    \begin{tabular}{|l|r|}
    \hline
    \bf Label & \bf Count \\
\hline
EGY   & 1578 \\
    GLF   & 1672 \\
    LAV   & 1758 \\
    MSA   & 999 \\
    NOR   & 1612 \\
    \hline
    Total & 7619 \\
    \hline
    \end{tabular}%
    \caption{Training data count}
  \label{tab:trainingLabels}%
 
\end{table}%

\subsection{Data pre-processing}
The data was provided in Bauckwalter format which uses English letters and symbols to represent pronunciation. We wrote a simple 1-to-1 mapping code to convert that back to Arabic letters so we can apply language dependent processing. For example "SHyfp Alr\textgreater y Al\textgreater rdnyp Elqt Alywm ElY mwDwE AHtmAl t\textgreater jyl AlAntxAbAt AlbrlmAnyp" appears as ``\<.s.hyfT Alra'y Ala'rdnyT `lqt Alywm `lY mw.dw` A.htmAl ta'jyl AlAntxAbAt AlbrlmAnyh>'' which translates to [Jordan's Al-Rai newspaper commented today on the possibility of postponing parliamentary elections].

\section{Machine Learning Algorithms}\label{alog}
For the purpose of this task we trained different text classifier using 4 algorithms:  Na\"{\i}ve Bayes, Support Vector Machine (SVM), k--Nearest Neighbor (KNN), Decision Trees (J48).

subsection{Baselines}\label{baselines}
As a baseline we created a simple classifier that always selects
the most frequent class (LAV in this case) with an accuracy of 23.01\%.
As an intelligent baseline we extracted word n-gram features selecting unigram, bigram and trigram contiguous words (n between $1$ and $3$). The classifiers' accuracy was low with only SVM reaching around 52\%. This was due to the high similarity between the dialects especially with short and one word instances. On average each instance contains 40 words with more than 3000 instances containing less than the 20. It is very difficult even for humans to guess the dialect for instances with one word such as ``\<n`m>'' [yes], ``\<ryA.dT>'' [sport] and ``\<t`lym>'' [education], a task that is deemed impossible for a machine since these words are used interchangeably between dialects and MSA. Therefore a more refined and language specific features are needed to help the machine distinguish between the dialects.

\subsection{Features Extraction}\label{featureSet}
We managed to increase prediction accuracy to near 90\% by selecting more language, stylistic and corpus-linguistics features. The selected features fall into three groups:
\begin{itemize}
\item Grammatical
\item Stylistic
\item Corpus-Linguistics
\end{itemize}

\subsubsection*{Grammatical Features}
In order to extract grammatical knowledge from the training data we used Stanford Part of Speech (POS) Tagger\footnote{\url{http://nlp.stanford.edu/software/tagger.shtml}} to annotate the text with grammatical part of speeches. The POS tagger was trained on MSA dataset but still works for dialects as even though dialects differ in pronunciations they still write similarly. The only difference will be in the structure of the sentences and the introduction of new words in dialects. Although this may make the tagger more error prone but it could help in distinguishing between dialects.

Using the annotated training data we extract features such as the tags frequency and uniqueness. We also extracted function words features such as: Adverbs, Adverbials, Conjunctions, demonstratives, modals, negations, particles, prepositional, prepositions, pronouns, quantities, question and relatives function words.

\subsubsection*{Stylistic and Contents Features}
In addition to grammatical features we also extracted stylistic features such as Type-Token-Ratio (TTR) used in authorship identification \cite{Holmes(1994)}.
TTR is the ratio obtained by dividing the total number of different words (i.e. types) occurring in a text by the total number of words (tokens). Higher TTR indicates a high degree of lexical variation. We calculated TTR by simply dividing the number of types by the number of tokens in each instance\cite{Holmes(1994)} : \[TTR = \frac{types}{tokens}\]

We normalised the output by dividing by the number of sentences in each instance, this was done by using Stanford Arabic sentence splitter\footnote{\url{http://nlp.stanford.edu/projects/arabic.shtml}}.

In addition to TTR we measured the readability of the text using OSMAN readability metric \cite{Elhaj(2016)}. In addition to providing a readability score between 0 (hard to read) to 100 (easy to read) OSMAN provide as well information about the number of syllables, hard words (\textgreater 5 words), complex words (\textgreater 4 syllables) and Faseeh (aspects of script usually dropped in informal Arabic writing).

And contents features by simply extracting word n-grams.

\subsubsection*{Corpus-Linguistics Features}
We used corpus-linguistics in order to study the closeness and homogeneity between the text of the dataset classes. As mentioned earlier the dataset contains a high level of code switching which is the practice of alternating between two ore more languages or dialects of the same language as in our case \cite{Elfardy(2014)}.

We used a corpus-linguistics approach to select features by creating frequency lists for each dialect in addition to MSA. As we noticed that dialect speakers have tend to switch code when trying to speak in MSA we worked on creating more specific frequency lists a) dialect frequency list minus MSA b) Frequency list of MSA vocabularies used in each dialect. We subtract the MSA frequency list from the lists of each dialect leaving us with more fine-grained dialectical lists as in Figure \ref{fig:1a}. We then subtracted the dialectical lists from the original frequency list of each dialect keeping the MSA vocabularies used in each dialect as in Figure \ref{fig:1b}.

\begin{figure}[h]
\centering
\begin{subfigure}{.50\textwidth}
  \centering
  \includegraphics[width=55mm, height=60mm]{freqLists.pdf}
  \caption{Dialect minus MSA} \label{fig:1a}
\end{subfigure}%
\begin{subfigure}{.50\textwidth}
  \centering
  \includegraphics[width=55mm, height=60mm]{freqLists2.pdf}
  \caption{MSA used in each dialect} \label{fig:1b}
\end{subfigure}
    \caption{Frequency lists}\label{fig:freqLists}
\end{figure}

\section{Features Reduction}\label{featureReduction}
The count of the selected features including each entry of the frequency and grammatical lists was 2559 divided into 3 categories as in Subsection \ref{featureSet}. Training the algorithms using this number of features took a significant amount of time making it difficult to try different algorithms. In order to simplify the model, shorten the training time and enhance generalisation to reduce over-fitting we reduced the features using two methods: a) Information Gain Ratio and b) Feature-Group filtering.

\subsection{a) Information Gain Ratio}\label{InfoGain}
We used feature evaluator and a ranker to rank the entire feature set. This was done through calculating Information Gain (InfoGrain) ratio which evaluates the worth of an attribute (feature contribution towards classification) by measuring InfoGain with respect to the class. This was combined with a ranker algorithm which ranks features by their individual evaluations (i.e. InfoGain). 

In order to reduce the number of features we needed to agree on a threshold and basically select the top $n$ relevant features to be used in the model construction.
Using the complete set of features the best algorithm (J48) achieved an accuracy of 88.9\%, we used this accuracy as a threshold to automatically train a decision tree and a SVM models separately to select top $n$ features starting with the total number of features and decreasing $n$ by 1 in each iteration until each model reaches near the required accuracy. The training stops when the accuracy drops. We found that the top 780 features can achieve an accuracy of 88.86\% using J48. This means that 30\% of the features were enough to achieve a similar accuracy of that using all the features combined. This 70\% reduction helped in simplifying the model and shortening the training time which helped us in building more models to test which algorithm provide better accuracy for dialect identification. 

\subsection{b) Feature-Group Filtering}\label{GroupFiltering}
In order to examine the effect of each features group (Subsection \ref{featureSet}) we ran the classifiers using all the training data testing each feature-group individually and combined. This process helps in determining which set of features (feature-group) could help in neatly splitting the dataset and thefore achieving high accuracy. It also helps to show which feature-groups are not meant to be used together, as such combination may increase complexity. The results of this process are shown details in Section \ref{results}. 

\section*{Results and Discussion}\label{results}

We show the classification results using our two feature reduction methods mentioned in Section \ref{featureReduction}.

\subsection{Baseline}
As mentioned earlier we used the most frequent class as our baseline model. We also used a unigram bigram and trigram word n-gram classifier as our intelligent baseline. Table \ref{tab:baselines} show the accuracy scores of each baseline.


\begin{table}[ht]
  \centering
    \begin{tabular}{|l|r|}
    \hline
    \bf Baseline & \bf Accuracy \\
\hline
Most frequent class   & 23.01\% \\
Word n-gram   & 52.53 \\
    \hline
    \end{tabular}%
    \caption{Baselines}
  \label{tab:baselines}%
 
\end{table}%

\subsection*{Training Results}
We used a 10 fold cross validation to evaluate the models and avoid over-fitting. As show in Section \ref{alog} we used 4 classifiers: J48,SVM, Na\"{\i}ve Bayes and KNN. 

% Results using reduced features (all training data)
\begin{table}[ht]
  \centering
    \begin{tabular}{|l|r|r|r|r|r|}
    \hline
    \bf Model & \bf Accuracy & \bf Recall & \bf Precision & \bf F-Measure & \bf Kappa \\
    \hline
    \bf J48   & \bf 88.86\% & \bf 0.89  & \bf 0.89  & \bf 0.89  & \bf 0.86 \\
    SVM   & 67.71\% & 0.68  & 0.73  & 0.69  & 0.59 \\
    Na\"{\i}ve Bayes & 44.00\% & 0.44  & 0.55  & 0.42  & 0.29 \\
   KNN   & 35.85\% & 0.36  & 0.37  & 0.36  & 0.19 \\
    \hline
    \end{tabular}%
      \caption{Results using InfoGain reduced features (all training data)}
  \label{tab:resultsAll}%
\end{table}%

Table \ref{tab:resultsAll} show the accuracy of the models when trained using all instances in the training dataset. This includes short and one word instances. A InfoGain filter (Subsection \ref{InfoGain}) was used to reduce the number of features.
Decision trees J48 achieved high accuracy with high recall and precision compared to the other algorithms. With more than 88\% accuracy our model with the selected features performed better than \cite{Ali(2015)} who used a set of lexical and acoustic features to train a classifier using similar dataset generated using the same ASR system.

To prove that filter reduction help in better splitting the dataset and thus making it easier for the algorithms to learn and distinguish between different classes we used a second level of features reduction.
Table \ref{tab:groups} shows the effect of using our group-feature reduction method (Subsection \ref{GroupFiltering}. For comparison purposes the first row in the table shows the result of the word n-gram baseline mentioned in Subsection \ref{baselines}.

% Table generated by Excel2LaTeX from sheet 'Sheet5'
\begin{table}[htbp]
  \centering
    \begin{tabular}{|l|r|r|r|r|}
    \hline
    \bf Feature(s) & \bf J48   & \bf SVM   & \bf NaiveBayes & \bf KNN \\
    \hline
    Word n-grams & 43.11 & 52.54 & 42.59 & 33.54 \\
    Stylistics & 42.82 & 52.91 & 42.16 & 33.25 \\
    Grammatical & 29.03 & 29.47 & 27.66 & 26.76 \\
    Corpus Linguistics & \bf 89.98 & 56.04 & 47.86 & \bf 89.03 \\
    Stylistics + Grammatical & 41.32 & 54.35 & 39.44 & 33.90 \\
    Stylistics + Corpus Linguistics & \bf 89.33 & 65.65 & 47.25 & 36.77 \\
    Corpus Linguistics + Grammatical & \bf 88.77 & 65.19 & 41.27 & 56.45 \\
    \hline
    \end{tabular}%
      \caption{Examining Features Groups}
  \label{tab:groups}%
\end{table}%

The results show that our Corpus Linguistics features alone outperform all the other features combined. 

We believe that our model can perform better with more refinement to the training data which could help decrease the impurity of the instances. As mentioned in Section \ref{alog} shorter text and one word instances could be difficult to identify even for  native speaker.
To tackle this problem we trained the system using the same set of refined features but this time only considering instances with more than or equal to 20 words. We reached this threshold by training a machine to increase the threshold with an increment of 1 for each classification iteration and to stop when the accuracy stops increasing or stalls. As shown in Table \ref{tab:resultsMore20} this has helped increasing the accuracy of the J48 classifier to around 98\%.


% Results using reduced features (instances \textgreater=20 words)
\begin{table}[h]
  \centering
    \begin{tabular}{|l|r|r|r|r|r|}
    \hline
    \bf Model & \bf Accuracy & \bf Recall & \bf Precision & \bf F-Measure & \bf Kappa \\
    \hline
    \bf J48   & \bf 97.75\%  & \bf 0.98  & \bf 0.98  & \bf 0.98  & \bf 0.97 \\
    SVM   & 83.54\%  & 0.84  & 0.84  & 0.84  & 0.79 \\
    Na\"{\i}ve Bayes & 54.03\%  & 0.54  & 0.61  & 0.54  & 0.43 \\
   KNN   & 40.96\%  & 0.41  & 0.43  & 0.41  & 0.26 \\
    \hline
    \end{tabular}%
      \caption{Results using reduced features (instances \textgreater=20 words)}
  \label{tab:resultsMore20}%
\end{table}%

By refining the training data by only selecting instances with more than 20 words we intended to show how words overlap between dialects. Having these words out of context or stand alone makes it difficult for a prediction system to infer the correct dialect even with more instances that we have in our training data.

% Results using reduced features (instances \textless20 words)
% \begin{table}[h]
%   \centering
%     \begin{tabular}{|l|r|r|r|r|r|}
%     \hline
%     Model & Accuracy & Recall & Precision & F-Measure & Kappa \\
%     \hline
%     \bf SVM   & \bf 76.34\% & \bf 0.76  & \bf 0.77  & \bf 0.76  & \bf 0.69 \\
%    	J48   & 75.87\% & 0.76  & 0.76  & 0.76  & 0.69 \\
%     Na\"{\i}ve Bayes & 54.08\% & 0.54  & 0.59  & 0.54  & 0.41 \\
% 	KNN   & 37.95\% & 0.38  & 0.38  & 0.38  & 0.19 \\
%     \hline
%     \end{tabular}%
%       \caption{Results using reduced features (instances \textless20 words)}
%   \label{tab:resultsLess20}%
% \end{table}%

Table \ref{tab:confMatrixAll} shows the confusion matrix for the J48 classifier which achieved the highest score as shown in Table \ref{tab:resultsAll}. The confusion matrix shows that some dialects are closer to each other which justifies our conclusion before that dialects use words interchangeably. The table shows GLF, LAV and NOR to be close to each other with considering the number of misclassified instances between them. The table also shows less confusion between dialects and MSA. We believe this is due to the use of frequency lists which made it easier for the classifier to distinguish between dialects and pure MSA. It is not quite clear which dialect is close to MSA but from the table we can guess that GLF is most likely closer to MSA than other dialects. Finally, it is important to note that our classifier has managed to equally distinguish between dialects where as we can see in the confusion matrix non of the dialects has been individually highly misclassified. This proves the selected features to be of good quality.

% Confusion Matrix
\begin{table}[h]
  \centering
    \begin{tabular}{|l|r|r|r|r|r|}
    \hline
          & \bf EGY   & \bf GLF   & \bf LAV   & \bf MSA   & \bf NOR \\
   \hline
    EGY   & \textbf{1424} & 56    & 41    & 15    & 40 \\
    GLF   & 42    & \textbf{1450} & 71    & 28    & 79 \\
    LAV   & 47    & 99    & \textbf{1506} & 14    & 83 \\
    MSA   & 15    & 15    & 13    & \textbf{952} & 4 \\
    NOR   & 19    & 84    & 66    & 16    & \textbf{1423} \\
    \hline
    Totals & 1547  & 1704  & 1697  & 1025  & 1629 \\

   \hline
    \end{tabular}%
      \caption{J48 Classifier Confusion Matrix (all training data)}
  \label{tab:confMatrixAll}%
\end{table}%

% % Confusion Matrix
% \begin{table}[h]
%   \centering
%     \begin{tabular}{|l|r|r|r|r|r|}
%     \hline
%           & EGY   & GLF   & LAV   & MSA   & NOR \\
%    \hline
%     EGY   & 1081  & 18    & 7     & 5     & 6 \\
%     GLF   & 4     & 803   & 6     & 3     & 8 \\
%     LAV   & 6     & 13    & 891   & 3     & 6 \\
%     MSA   & 0     & 1     & 0     & 751   & 0 \\
%     NOR   & 2     & 8     & 2     & 2     & 827 \\
%    \hline
%     \end{tabular}%
%       \caption{J48 Classifier Confusion Matrix (instances \textgreater=20 words)}
%   \label{tab:confMatrixMore}%
% \end{table}%



% % Confusion Matrix
% \begin{table}[h]
%   \centering
%     \begin{tabular}{|l|r|r|r|r|r|}
%     \hline
%           & EGY   & GLF   & LAV   & MSA   & NOR \\
%    \hline
%     EGY   & 314   & 50    & 39    & 11    & 45 \\
%     GLF   & 32    & 667   & 65    & 14    & 68 \\
%     LAV   & 36    & 94    & 594   & 21    & 85 \\
%     MSA   & 9     & 6     & 10    & 214   & 8 \\
%     NOR   & 25    & 67    & 60    & 15    & 600 \\
%    \hline
%     \end{tabular}%
%       \caption{J48 Classifier Confusion Matrix (instances \textless20 words)}
%   \label{tab:confMatrixLess}%
% \end{table}%

\subsection*{Testing Results}
MO: I'll update this when I classify the testing data (we should get those by the end of August).

\section*{Acknowledgment}

Thanks for University Centre for Computer Corpus Research on Language (UCREL) for funding this work.

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{references}


\end{document}
